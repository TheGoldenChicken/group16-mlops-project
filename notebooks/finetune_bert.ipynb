{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,TrainingArguments,set_seed\n",
    "from peft import LoraConfig, PeftModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "hf_token = \"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\"\n",
    "wandb_key = \"7ea086a098e40728fdf48b616051776a17daf566\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"MLOps-Project\"\n",
    "\n",
    "#monitering login\n",
    "wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_model(model_name, dataset, r, epochs, lr):\n",
    "    # Load base model(code-llama-7b) and tokenizer\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit= True,\n",
    "        bnb_4bit_quant_type= \"nf4\",\n",
    "        bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant= False,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0}\n",
    "    )\n",
    "    model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    # Load LLaMA tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.add_eos_token = True\n",
    "    tokenizer.add_bos_token, tokenizer.add_eos_token\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha = 8,\n",
    "        lora_dropout = 0.1,\n",
    "        r = r,\n",
    "        bias = \"none\",\n",
    "        task_type = \"CAUSAL_LM\",\n",
    "        layers_to_transform = [i for i in range(10, 32)]\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir= \"./results\",\n",
    "        num_train_epochs= epochs, # 1\n",
    "        per_device_train_batch_size= 2,\n",
    "        gradient_accumulation_steps= 2,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        save_steps= 100,\n",
    "        logging_steps= 10, # 10,\n",
    "        learning_rate= lr,\n",
    "        weight_decay= 0.001,\n",
    "        fp16= False,\n",
    "        bf16= False,\n",
    "        max_grad_norm= 0.3,\n",
    "        max_steps= -1,\n",
    "        warmup_ratio= 0.2, # 0.3\n",
    "        group_by_length= True,\n",
    "        lr_scheduler_type= \"linear\", # \"constant\"\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"r={r} epochs={epochs} lr={lr}\"\n",
    "    )\n",
    "\n",
    "    # Setting sft parameters\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length= None,\n",
    "        dataset_text_field=\"premise\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing= False,\n",
    "    )\n",
    "\n",
    "    print(f\"Hyperparameters: r = {r}, epochs = {epochs}\")\n",
    "    print(\"Ready to train\")\n",
    "\n",
    "    return trainer, model\n",
    "\n",
    "def save_model(trainer, model, save_directory, finetune_name):\n",
    "    # Save the fine-tuned model in directory\n",
    "    trainer.model.save_pretrained(save_directory + \"/\" + finetune_name)\n",
    "    wandb.finish()\n",
    "    model.config.use_cache = True\n",
    "    model.eval()\n",
    "\n",
    "def load_finetuned_model(base_model_name, model_directory, finetune_name):\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map= {\"\": 0})\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, model_directory + \"/\" + finetune_name)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    # Reload tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def upload_to_huggingface(model, tokenizer, finetuned_model_name):\n",
    "    # Upload model to huggingface\n",
    "    model.push_to_hub(finetuned_model_name, use_auth_token=hf_token)\n",
    "    tokenizer.push_to_hub(finetuned_model_name, use_auth_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
    "dataset_name = \"snli\"\n",
    "finetuned_model_name = \"magnus42/MLOps-Projekt\"\n",
    "save_directory = \"models\" #\"/work3/s204164/LLAMA2_Finetuning/trained_models/final_model\"\n",
    "train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 0.01\n",
    "r = 16\n",
    "seed = 42\n",
    "\n",
    "set_seed(seed)\n",
    "finetune_name = f\"finetuned_lr{lr}_e{epochs}_r{r}_seed{seed}\"\n",
    "trainer, model = reload_model(model_name, train_dataset, r, epochs, lr)\n",
    "trainer.train()\n",
    "save_model(trainer, model, save_directory, finetune_name)\n",
    "upload_to_huggingface(model_name, save_directory, finetune_name)\n",
    "# Clear the memory footprint\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"This script has finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
